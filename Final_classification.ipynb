{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-WttHYyhXuz"
      },
      "source": [
        "## Sixth Session (Related to the Course Project)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgciSSBhXu0"
      },
      "source": [
        "---------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW5qKxrjhXu0"
      },
      "source": [
        "## Graph Classification with [Deep Graph Library (DGL)](https://docs.dgl.ai/index.html) for the graduate course \"[Graph Machine learning](https://github.com/zahta/graph_ml)\"\n",
        "\n",
        "### Dataset: bbbp\n",
        "\n",
        "##### by [Zahra Taheri](https://github.com/zahta), 06 June 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77T5Lv2hhXu0"
      },
      "source": [
        "---------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoTzK-2IhXu1"
      },
      "source": [
        "### This Tutorial Is Prepared Based on the Following References\n",
        "\n",
        "- [FunQG: Molecular Representation Learning via Quotient Graphs](https://pubs.acs.org/doi/10.1021/acs.jcim.3c00445)\n",
        "- [Supporting Information of FunQG](https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c00445/suppl_file/ci3c00445_si_001.pdf)\n",
        "- [GitHub Repository of FunQG](https://github.com/hhaji/funqg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install  dgl -f https://data.dgl.ai/wheels/repo.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5IQfzqdbMY4",
        "outputId": "12de1b8b-f62a-446b-eaa5-17d2de890955"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl\n",
            "  Downloading dgl-1.1.1-cp310-cp310-manylinux1_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install  dgl -f https://data.dgl.ai/wheels/cu113/repo.html"
      ],
      "metadata": {
        "id": "rgKaKy-sha0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html"
      ],
      "metadata": {
        "id": "03IbWGqfhlRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTEcaNXsbQFp",
        "outputId": "21a9f36b-dc8e-47f9-b26d-64cb9643b9a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
            "Collecting dglgo\n",
            "  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (0.7.0)\n",
            "Collecting isort>=5.10.1 (from dglgo)\n",
            "  Downloading isort-5.12.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autopep8>=1.6.0 (from dglgo)\n",
            "  Downloading autopep8-2.0.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpydoc>=1.1.0 (from dglgo)\n",
            "  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.10.9)\n",
            "Collecting ruamel.yaml>=0.17.20 (from dglgo)\n",
            "  Downloading ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from dglgo) (6.0)\n",
            "Collecting ogb>=1.3.3 (from dglgo)\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit-pypi (from dglgo)\n",
            "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.2.2)\n",
            "Collecting pycodestyle>=2.10.0 (from autopep8>=1.6.0->dglgo)\n",
            "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
            "Collecting sphinx>=4.2 (from numpydoc>=1.1.0->dglgo)\n",
            "  Downloading sphinx-7.0.1-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (4.65.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.26.16)\n",
            "Collecting outdated>=0.2.0 (from ogb>=1.3.3->dglgo)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.0->dglgo) (4.6.3)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.20->dglgo)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.4.0->dglgo) (8.1.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi->dglgo) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.3)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb>=1.3.3->dglgo)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2022.7.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.3)\n",
            "Requirement already satisfied: Pygments>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.14.0)\n",
            "Collecting docutils<0.21,>=0.18.1 (from sphinx>=4.2->numpydoc>=1.1.0->dglgo)\n",
            "  Downloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: snowballstemmer>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (0.7.13)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb>=1.3.3->dglgo) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb>=1.3.3->dglgo) (16.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb>=1.3.3->dglgo) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=d2e085dc6cc7e88a6a9c7ed3645b8a73da216f6dc67d6a34d2266825451754b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, ruamel.yaml.clib, rdkit-pypi, pycodestyle, isort, docutils, sphinx, ruamel.yaml, outdated, autopep8, numpydoc, ogb, dglgo\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.16\n",
            "    Uninstalling docutils-0.16:\n",
            "      Successfully uninstalled docutils-0.16\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 3.5.4\n",
            "    Uninstalling Sphinx-3.5.4:\n",
            "      Successfully uninstalled Sphinx-3.5.4\n",
            "Successfully installed autopep8-2.0.2 dglgo-0.0.2 docutils-0.20.1 isort-5.12.0 littleutils-0.2.2 numpydoc-1.5.0 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.10.0 rdkit-pypi-2022.9.5 ruamel.yaml-0.17.32 ruamel.yaml.clib-0.2.7 sphinx-7.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RH7Or5WjhXu1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl.function as fn\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "from torch.utils.data import DataLoader\n",
        "import cloudpickle\n",
        "from dgl.nn import GraphConv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from dgl.nn import GraphConv\n",
        "from dgl.nn import GINConv\n",
        "from dgl.nn import SAGEConv\n",
        "from dgl.nn import GATConv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z37D6dxGhXu1"
      },
      "source": [
        "#### Set Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g47gT5EGD_aX",
        "outputId": "a4feac66-50f0-42a2-f46e-e0ddadea1d29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unzip\n",
            "  Downloading unzip-1.0.0.tar.gz (704 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unzip\n",
            "  Building wheel for unzip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unzip: filename=unzip-1.0.0-py3-none-any.whl size=1279 sha256=8d395ed526a9d452eae06645515c273c9a8581b8f874fb62b2ccc2b2c9a9b111\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/dc/7a/f8af45bc239e7933509183f038ea8d46f3610aab82b35369f4\n",
            "Successfully built unzip\n",
            "Installing collected packages: unzip\n",
            "Successfully installed unzip-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc0AxQa33KPm",
        "outputId": "62d41cc4-397e-45ea-9cd3-76c495529215"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/graph_data1.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xewnPG0uELcs",
        "outputId": "f5cfcaf4-0d18-48c3-c786-c8452542c787"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/graph_data1.zip\n",
            "  inflating: scaffold_0_smiles_train.pickle  \n",
            "  inflating: scaffold_0_test.bin     \n",
            "  inflating: scaffold_0_val.bin      \n",
            "  inflating: scaffold_0_smiles_val.pickle  \n",
            "  inflating: scaffold_0_smiles_test.pickle  \n",
            "  inflating: scaffold_0_train.bin    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the current directory where the ZIP file is located.\n",
        "current_dir = \"/content/drive/MyDrive/graph_data1.zip\"\n",
        "\n",
        "# Create the path to the directory where model checkpoints will be saved.\n",
        "checkpoint_path = current_dir + \"save_models/model_checkpoints/\" + \"checkpoint\"\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the directory where the best model will be saved.\n",
        "best_model_path = current_dir + \"save_models/best_model/\"\n",
        "\n",
        "# Create a temporary folder path for data manipulation.\n",
        "folder_data_temp = current_dir + \"data_temp/\"\n",
        "\n",
        "# Remove the temporary folder if it exists, ignoring any errors if it does not.\n",
        "shutil.rmtree(folder_data_temp, ignore_errors=True)\n",
        "\n",
        "# Define the path to save the unpacked files from the ZIP archive.\n",
        "path_save = current_dir\n",
        "\n",
        "# Unpack the contents of the ZIP archive to the temporary folder.\n",
        "shutil.unpack_archive(path_save, folder_data_temp)\n"
      ],
      "metadata": {
        "id": "Oig_cfJnzMSZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4GH1uUxhXu2"
      },
      "source": [
        "#### Custom PyTorch Datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Classification Dataset \"\"\"\n",
        "\n",
        "class DGLDatasetClass(torch.utils.data.Dataset):\n",
        "    def __init__(self, address):\n",
        "        # Initialize the dataset with the given address\n",
        "        self.address = address + \".bin\"\n",
        "        self.list_graphs, train_labels_masks_globals = dgl.load_graphs(self.address)\n",
        "        num_graphs = len(self.list_graphs)\n",
        "\n",
        "        # Extract labels, masks, and globals from train_labels_masks_globals\n",
        "        self.labels = train_labels_masks_globals[\"labels\"].view(num_graphs, -1)\n",
        "        self.masks = train_labels_masks_globals[\"masks\"].view(num_graphs, -1)\n",
        "        self.globals = train_labels_masks_globals[\"globals\"].view(num_graphs, -1)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the length of the dataset\n",
        "        return len(self.list_graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve the item at the given index\n",
        "        return self.list_graphs[idx], self.labels[idx], self.masks[idx], self.globals[idx]\n"
      ],
      "metadata": {
        "id": "syHoPuso1b2e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39AeT4b4hXu2"
      },
      "source": [
        "#### Defining Train, Validation, and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path for temporary data\n",
        "path_data_temp = folder_data_temp + \"scaffold\" + \"_\" + str(0)\n",
        "\n",
        "# Create instances of DGLDatasetClass for the train, validation, and test sets\n",
        "train_set = DGLDatasetClass(address=path_data_temp + \"_train\")\n",
        "val_set = DGLDatasetClass(address=path_data_temp + \"_val\")\n",
        "test_set = DGLDatasetClass(address=path_data_temp + \"_test\")\n",
        "\n",
        "# Print the lengths of the train, validation, and test sets\n",
        "print(len(train_set), len(val_set), len(test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aISbVq8C2bJV",
        "outputId": "c03fe86a-f40a-44e2-e827-67a1b595ec11"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1631 203 205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN3c53dRhXu3"
      },
      "source": [
        "#### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a collate function to process a batch of data samples.\n",
        "def collate(batch):\n",
        "    # Extract the graphs from the batch and create a batched graph using dgl.batch.\n",
        "    graphs = [e[0] for e in batch]\n",
        "    g = dgl.batch(graphs)\n",
        "\n",
        "    # Extract the labels from the batch and stack them into a tensor.\n",
        "    labels = [e[1] for e in batch]\n",
        "    labels = torch.stack(labels, 0)\n",
        "\n",
        "    # Extract the masks from the batch and stack them into a tensor.\n",
        "    masks = [e[2] for e in batch]\n",
        "    masks = torch.stack(masks, 0)\n",
        "\n",
        "    # Extract the global features from the batch and stack them into a tensor.\n",
        "    globals = [e[3] for e in batch]\n",
        "    globals = torch.stack(globals, 0)\n",
        "\n",
        "    # Return the batched graph, labels, masks, and globals.\n",
        "    return g, labels, masks, globals\n",
        "\n",
        "\n",
        "# Define a loader function to create data loaders for the training, validation, and test sets.\n",
        "def loader(batch_size=64):\n",
        "    # Create a data loader for the training set.\n",
        "    train_dataloader = DataLoader(train_set,\n",
        "                                  batch_size=batch_size,\n",
        "                                  collate_fn=collate,\n",
        "                                  drop_last=False,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=1)\n",
        "\n",
        "    # Create a data loader for the validation set.\n",
        "    val_dataloader = DataLoader(val_set,\n",
        "                                batch_size=batch_size,\n",
        "                                collate_fn=collate,\n",
        "                                drop_last=False,\n",
        "                                shuffle=False,\n",
        "                                num_workers=1)\n",
        "\n",
        "    # Create a data loader for the test set.\n",
        "    test_dataloader = DataLoader(test_set,\n",
        "                                 batch_size=batch_size,\n",
        "                                 collate_fn=collate,\n",
        "                                 drop_last=False,\n",
        "                                 shuffle=False,\n",
        "                                 num_workers=1)\n",
        "\n",
        "    # Return the data loaders for training, validation, and test sets.\n",
        "    return train_dataloader, val_dataloader, test_dataloader\n"
      ],
      "metadata": {
        "id": "lBsQwY2D5vqD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zHCkULH5hXu3"
      },
      "outputs": [],
      "source": [
        "# Create data loaders for the training, validation, and test sets with a batch size of 64.\n",
        "train_dataloader, val_dataloader, test_dataloader = loader(batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBRkhkI3hXu3"
      },
      "source": [
        "#### Defining A GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDru1hnyhXu3"
      },
      "source": [
        "##### Some Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CFPaFdHkhXu3"
      },
      "outputs": [],
      "source": [
        "#Bace dataset has 1 task. Some other datasets may have some more number of tasks, e.g., tox21 has 12 tasks.\n",
        "num_tasks = 1\n",
        "\n",
        "# Size of global feature of each graph\n",
        "global_size = 200\n",
        "\n",
        "# Number of epochs to train the model\n",
        "num_epochs = 100\n",
        "\n",
        "# Number of steps to wait if the model performance on the validation set does not improve\n",
        "patience = 10\n",
        "\n",
        "#Configurations to instantiate the model\n",
        "config = {\"node_feature_size\":127, \"edge_feature_size\":12, \"hidden_size\":100}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoPRoiI5hXu3"
      },
      "outputs": [],
      "source": [
        "#Define a GNN (Graph Neural Network) class as a subclass of nn.Module\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        # Create the first GraphConv layer with input size equal to the node feature size and output size equal to the hidden size.\n",
        "        self.conv1 = GraphConv(self.node_feature_size, self.hidden_size, allow_zero_in_degree = True)\n",
        "\n",
        "        # Create the second GraphConv layer with input size equal to the hidden size and output size equal to the number of tasks\n",
        "        self.conv2 = GraphConv(self.hidden_size, self.num_tasks, allow_zero_in_degree = True)\n",
        "\n",
        "    # def forward(self, g, in_feat):\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzqdwrdohXu4"
      },
      "source": [
        "#### Function to Compute Score of the Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Define the metric as roc_auc_score\n",
        "    metric = roc_auc_score\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Initialize empty tensors for storing predictions, labels, and masks\n",
        "        prediction_all = torch.empty(0)\n",
        "        labels_all = torch.empty(0)\n",
        "        masks_all = torch.empty(0)\n",
        "\n",
        "        # Iterate over the data_loader\n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            # Compute predictions from the model\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction = torch.sigmoid(prediction)\n",
        "\n",
        "            # Concatenate predictions, labels, and masks to the respective tensors\n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "\n",
        "        # Initialize average score tensor\n",
        "        average = torch.tensor([0.])\n",
        "\n",
        "        # Compute the metric for each task\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:, i] == 1]\n",
        "            a2 = labels_all[:, i][masks_all[:, i] == 1]\n",
        "\n",
        "            try:\n",
        "                # Calculate the metric score\n",
        "                t = metric(a2.int().cpu(), a1.cpu()).item()\n",
        "            except ValueError:\n",
        "                # Handle the case where the metric calculation throws a ValueError\n",
        "                t = 0\n",
        "\n",
        "            average += t\n",
        "\n",
        "    # Return the average score divided by the number of tasks\n",
        "    return average.item() / num_tasks\n"
      ],
      "metadata": {
        "id": "U1pu-l6hSvvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNOajUO7hXu4"
      },
      "source": [
        "#### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIa0EBikhXu4"
      },
      "outputs": [],
      "source": [
        "# Define the loss function\n",
        "def loss_func(output, label, mask, num_tasks):\n",
        "    # Create a tensor of ones as positive weights for BCEWithLogitsLoss\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "\n",
        "    # Define the criterion as BCEWithLogitsLoss with no reduction and positive weights\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
        "\n",
        "    # Compute the element-wise loss by applying the mask to the criterion output\n",
        "    loss = mask * criterion(output, label)\n",
        "\n",
        "    # Compute the average loss by summing the masked loss values and dividing by the sum of the mask values\n",
        "    loss = loss.sum() / mask.sum()\n",
        "\n",
        "    # Return the computed loss\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8aquB5ehXu4"
      },
      "source": [
        "#### Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fgQkoc7hXu4"
      },
      "source": [
        "##### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmR38IFxhXu4"
      },
      "outputs": [],
      "source": [
        "# Define a function to train a single epoch using the given training data loader, model, and optimizer.\n",
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    # Initialize the epoch train loss and iterations.\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "\n",
        "    # Set the model to train mode.\n",
        "    model.train()\n",
        "\n",
        "    # Iterate over the training data loader.\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        # Make predictions using the model.\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "\n",
        "        # Compute the training loss using the loss function.\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "\n",
        "        # Zero the gradients of the model parameters.\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Perform backpropagation to compute gradients.\n",
        "        loss_train.backward()\n",
        "\n",
        "        # Update the model parameters using the optimizer.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the training loss.\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "\n",
        "        # Increment the iterations count.\n",
        "        iterations += 1\n",
        "\n",
        "    # Compute the average epoch train loss.\n",
        "    epoch_train_loss /= iterations\n",
        "\n",
        "    # Return the average epoch train loss.\n",
        "    return epoch_train_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx02YIrihXu5"
      },
      "outputs": [],
      "source": [
        "# Define a function to train and evaluate the model.\n",
        "def train_evaluate():\n",
        "    # Create a new instance of the GNN model with the given configuration.\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "\n",
        "    # Create an Adam optimizer for training the model.\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    # Initialize variables for tracking the best validation score and patience count.\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    # Continue training until reaching the maximum number of epochs.\n",
        "    while epoch <= num_epochs:\n",
        "        # Check if the patience count is within the allowed limit.\n",
        "        if patience_count <= patience:\n",
        "            # Set the model to train mode and compute the training loss for the current epoch.\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "\n",
        "            # Set the model to eval mode and compute the validation score.\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "\n",
        "            # Check if the current validation score is better than the best validation score so far.\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "\n",
        "                # Create a dictionary to store the checkpoint information.\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "\n",
        "                # Save the checkpoint to a file using cloudpickle.\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            # Print the training and validation scores for the current epoch.\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(epoch, num_epochs, loss_train, score_val))\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "    # Save the best model by copying the checkpoint directory.\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    # Print the final results.\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twE7VJvQhXu5"
      },
      "source": [
        "##### Function to compute test set score of the final saved model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OQblpT0hXu5"
      },
      "source": [
        "##### Train the model and evaluate its performance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluate():\n",
        "    # Create the final model\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "\n",
        "    # Set the path to the best model checkpoint file\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "\n",
        "    # Open the best model checkpoint file and load it\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "\n",
        "    # Load the state dictionary of the best model\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # Set the final model to evaluation mode\n",
        "    final_model.eval()\n",
        "\n",
        "    # Compute the test score\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    # Print the test score\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "\n",
        "    # Print the execution time\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"
      ],
      "metadata": {
        "id": "2Ivm_9-PIRxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11d-SdbahXu5",
        "outputId": "f858cdf2-0483-4662-bd52-475ea4350baf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.695 | Valid Score: 0.285\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.285 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 2/100 | Training Loss: 0.657 | Valid Score: 0.255\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.285 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 3/100 | Training Loss: 0.633 | Valid Score: 0.255\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.285 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 4/100 | Training Loss: 0.615 | Valid Score: 0.262\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.285 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 5/100 | Training Loss: 0.605 | Valid Score: 0.269\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.285 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 6/100 | Training Loss: 0.599 | Valid Score: 0.278\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.285 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 7/100 | Training Loss: 0.594 | Valid Score: 0.287\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.287 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 0.590 | Valid Score: 0.300\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.300 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 9/100 | Training Loss: 0.587 | Valid Score: 0.315\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.315 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 0.583 | Valid Score: 0.332\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.332 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 11/100 | Training Loss: 0.581 | Valid Score: 0.347\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.347 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 12/100 | Training Loss: 0.572 | Valid Score: 0.365\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 0.365 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 13/100 | Training Loss: 0.570 | Valid Score: 0.387\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 0.387 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 14/100 | Training Loss: 0.567 | Valid Score: 0.416\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 0.416 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 15/100 | Training Loss: 0.563 | Valid Score: 0.451\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 0.451 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 16/100 | Training Loss: 0.561 | Valid Score: 0.473\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 0.473 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 17/100 | Training Loss: 0.556 | Valid Score: 0.512\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 0.512 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 18/100 | Training Loss: 0.552 | Valid Score: 0.541\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 0.541 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 19/100 | Training Loss: 0.548 | Valid Score: 0.563\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 0.563 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 20/100 | Training Loss: 0.546 | Valid Score: 0.591\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 0.591 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 21/100 | Training Loss: 0.543 | Valid Score: 0.609\n",
            " \n",
            "Epoch: 21/100 | Best Valid Score Until Now: 0.609 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 22/100 | Training Loss: 0.539 | Valid Score: 0.638\n",
            " \n",
            "Epoch: 22/100 | Best Valid Score Until Now: 0.638 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 23/100 | Training Loss: 0.535 | Valid Score: 0.666\n",
            " \n",
            "Epoch: 23/100 | Best Valid Score Until Now: 0.666 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 24/100 | Training Loss: 0.530 | Valid Score: 0.682\n",
            " \n",
            "Epoch: 24/100 | Best Valid Score Until Now: 0.682 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 25/100 | Training Loss: 0.530 | Valid Score: 0.692\n",
            " \n",
            "Epoch: 25/100 | Best Valid Score Until Now: 0.692 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 26/100 | Training Loss: 0.526 | Valid Score: 0.715\n",
            " \n",
            "Epoch: 26/100 | Best Valid Score Until Now: 0.715 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 27/100 | Training Loss: 0.525 | Valid Score: 0.727\n",
            " \n",
            "Epoch: 27/100 | Best Valid Score Until Now: 0.727 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 28/100 | Training Loss: 0.519 | Valid Score: 0.739\n",
            " \n",
            "Epoch: 28/100 | Best Valid Score Until Now: 0.739 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 29/100 | Training Loss: 0.517 | Valid Score: 0.744\n",
            " \n",
            "Epoch: 29/100 | Best Valid Score Until Now: 0.744 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 30/100 | Training Loss: 0.516 | Valid Score: 0.749\n",
            " \n",
            "Epoch: 30/100 | Best Valid Score Until Now: 0.749 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 31/100 | Training Loss: 0.514 | Valid Score: 0.758\n",
            " \n",
            "Epoch: 31/100 | Best Valid Score Until Now: 0.758 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 32/100 | Training Loss: 0.511 | Valid Score: 0.767\n",
            " \n",
            "Epoch: 32/100 | Best Valid Score Until Now: 0.767 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 33/100 | Training Loss: 0.506 | Valid Score: 0.774\n",
            " \n",
            "Epoch: 33/100 | Best Valid Score Until Now: 0.774 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 34/100 | Training Loss: 0.507 | Valid Score: 0.778\n",
            " \n",
            "Epoch: 34/100 | Best Valid Score Until Now: 0.778 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 35/100 | Training Loss: 0.501 | Valid Score: 0.779\n",
            " \n",
            "Epoch: 35/100 | Best Valid Score Until Now: 0.779 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 36/100 | Training Loss: 0.500 | Valid Score: 0.780\n",
            " \n",
            "Epoch: 36/100 | Best Valid Score Until Now: 0.780 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 37/100 | Training Loss: 0.496 | Valid Score: 0.786\n",
            " \n",
            "Epoch: 37/100 | Best Valid Score Until Now: 0.786 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 38/100 | Training Loss: 0.495 | Valid Score: 0.786\n",
            " \n",
            "Epoch: 38/100 | Best Valid Score Until Now: 0.786 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 39/100 | Training Loss: 0.493 | Valid Score: 0.792\n",
            " \n",
            "Epoch: 39/100 | Best Valid Score Until Now: 0.792 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 40/100 | Training Loss: 0.491 | Valid Score: 0.792\n",
            " \n",
            "Epoch: 40/100 | Best Valid Score Until Now: 0.792 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 41/100 | Training Loss: 0.488 | Valid Score: 0.795\n",
            " \n",
            "Epoch: 41/100 | Best Valid Score Until Now: 0.795 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 43/100 | Training Loss: 0.484 | Valid Score: 0.799\n",
            " \n",
            "Epoch: 43/100 | Best Valid Score Until Now: 0.799 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 44/100 | Training Loss: 0.484 | Valid Score: 0.800\n",
            " \n",
            "Epoch: 44/100 | Best Valid Score Until Now: 0.800 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 45/100 | Training Loss: 0.481 | Valid Score: 0.801\n",
            " \n",
            "Epoch: 45/100 | Best Valid Score Until Now: 0.801 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 46/100 | Training Loss: 0.480 | Valid Score: 0.804\n",
            " \n",
            "Epoch: 46/100 | Best Valid Score Until Now: 0.804 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 47/100 | Training Loss: 0.477 | Valid Score: 0.805\n",
            " \n",
            "Epoch: 47/100 | Best Valid Score Until Now: 0.805 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 48/100 | Training Loss: 0.476 | Valid Score: 0.805\n",
            " \n",
            "Epoch: 48/100 | Best Valid Score Until Now: 0.805 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 49/100 | Training Loss: 0.474 | Valid Score: 0.808\n",
            " \n",
            "Epoch: 49/100 | Best Valid Score Until Now: 0.808 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 50/100 | Training Loss: 0.473 | Valid Score: 0.808\n",
            " \n",
            "Epoch: 50/100 | Best Valid Score Until Now: 0.808 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 51/100 | Training Loss: 0.470 | Valid Score: 0.810\n",
            " \n",
            "Epoch: 51/100 | Best Valid Score Until Now: 0.810 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 52/100 | Training Loss: 0.471 | Valid Score: 0.810\n",
            " \n",
            "Epoch: 52/100 | Best Valid Score Until Now: 0.810 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 53/100 | Training Loss: 0.471 | Valid Score: 0.812\n",
            " \n",
            "Epoch: 53/100 | Best Valid Score Until Now: 0.812 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 54/100 | Training Loss: 0.467 | Valid Score: 0.814\n",
            " \n",
            "Epoch: 54/100 | Best Valid Score Until Now: 0.814 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 55/100 | Training Loss: 0.463 | Valid Score: 0.812\n",
            " \n",
            "Epoch: 55/100 | Best Valid Score Until Now: 0.814 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 56/100 | Training Loss: 0.463 | Valid Score: 0.814\n",
            " \n",
            "Epoch: 56/100 | Best Valid Score Until Now: 0.814 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 57/100 | Training Loss: 0.461 | Valid Score: 0.814\n",
            " \n",
            "Epoch: 57/100 | Best Valid Score Until Now: 0.814 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 58/100 | Training Loss: 0.462 | Valid Score: 0.814\n",
            " \n",
            "Epoch: 58/100 | Best Valid Score Until Now: 0.814 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 59/100 | Training Loss: 0.462 | Valid Score: 0.814\n",
            " \n",
            "Epoch: 59/100 | Best Valid Score Until Now: 0.814 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 60/100 | Training Loss: 0.456 | Valid Score: 0.814\n",
            " \n",
            "Epoch: 60/100 | Best Valid Score Until Now: 0.814 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 61/100 | Training Loss: 0.459 | Valid Score: 0.815\n",
            " \n",
            "Epoch: 61/100 | Best Valid Score Until Now: 0.815 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 62/100 | Training Loss: 0.457 | Valid Score: 0.814\n",
            " \n",
            "Epoch: 62/100 | Best Valid Score Until Now: 0.815 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 63/100 | Training Loss: 0.453 | Valid Score: 0.815\n",
            " \n",
            "Epoch: 63/100 | Best Valid Score Until Now: 0.815 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 64/100 | Training Loss: 0.455 | Valid Score: 0.815\n",
            " \n",
            "Epoch: 64/100 | Best Valid Score Until Now: 0.815 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 65/100 | Training Loss: 0.455 | Valid Score: 0.816\n",
            " \n",
            "Epoch: 65/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 66/100 | Training Loss: 0.453 | Valid Score: 0.816\n",
            " \n",
            "Epoch: 66/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 67/100 | Training Loss: 0.450 | Valid Score: 0.816\n",
            " \n",
            "Epoch: 67/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 68/100 | Training Loss: 0.450 | Valid Score: 0.816\n",
            " \n",
            "Epoch: 68/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 69/100 | Training Loss: 0.450 | Valid Score: 0.816\n",
            " \n",
            "Epoch: 69/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 70/100 | Training Loss: 0.447 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 70/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 71/100 | Training Loss: 0.447 | Valid Score: 0.817\n",
            " \n",
            "Epoch: 71/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 72/100 | Training Loss: 0.449 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 72/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 73/100 | Training Loss: 0.443 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 73/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 74/100 | Training Loss: 0.443 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 74/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 75/100 | Training Loss: 0.443 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 75/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 76/100 | Training Loss: 0.441 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 76/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 77/100 | Training Loss: 0.443 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 77/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 78/100 | Training Loss: 0.442 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 78/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 79/100 | Training Loss: 0.442 | Valid Score: 0.820\n",
            " \n",
            "Epoch: 79/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 80/100 | Training Loss: 0.437 | Valid Score: 0.820\n",
            " \n",
            "Epoch: 80/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 81/100 | Training Loss: 0.439 | Valid Score: 0.820\n",
            " \n",
            "Epoch: 81/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 82/100 | Training Loss: 0.440 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 82/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 83/100 | Training Loss: 0.436 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 83/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 84/100 | Training Loss: 0.438 | Valid Score: 0.820\n",
            " \n",
            "Epoch: 84/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 85/100 | Training Loss: 0.435 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 85/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 86/100 | Training Loss: 0.438 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 86/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 87/100 | Training Loss: 0.438 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 87/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 88/100 | Training Loss: 0.436 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 88/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 89/100 | Training Loss: 0.435 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 89/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 90/100 | Training Loss: 0.433 | Valid Score: 0.820\n",
            " \n",
            "Epoch: 90/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 91/100 | Training Loss: 0.434 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 91/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 92/100 | Training Loss: 0.434 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 92/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 93/100 | Training Loss: 0.429 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 93/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.822 \n",
            "\n",
            "Test Score: 0.612 \n",
            "\n",
            "Execution time: 55.808 seconds\n"
          ]
        }
      ],
      "source": [
        "#This line imports the time module, which provides various time-related functions\n",
        "import time\n",
        "#This line records the current time using time.time() and assigns it to the variable start_time. It serves as the starting point for measuring the execution time\n",
        "start_time = time.time()\n",
        "#This line calls the train_evaluate() function, which is likely responsible for training and evaluating a model.\n",
        "train_evaluate()\n",
        "#This line calls the test_evaluate() function, which probably performs evaluation on a separate test dataset.\n",
        "test_evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GNN 2"
      ],
      "metadata": {
        "id": "2q1e8nb-TafH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = SAGEConv(self.node_feature_size, self.hidden_size, aggregator_type='mean')\n",
        "        self.conv2 = SAGEConv(self.hidden_size, self.num_tasks, aggregator_type='mean')\n",
        "\n",
        "    # def forward(self, g, in_feat):\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ],
      "metadata": {
        "id": "v6Ts7zalTjlf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "    # A function to compute the performance score of the model on the given data\n",
        "    # model: the trained model to be evaluated\n",
        "    # data_loader: the data loader for the validation or test set\n",
        "    model.eval()\n",
        "    metric = roc_auc_score\n",
        "    with torch.no_grad():\n",
        "        # Initialize empty tensors to store the predictions, labels, and masks\n",
        "        prediction_all= torch.empty(0)\n",
        "        labels_all= torch.empty(0)\n",
        "        masks_all= torch.empty(0)\n",
        "        # Iterate over the data loader and make predictions for each batch\n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction = torch.sigmoid(prediction)\n",
        "            # Concatenate the predictions, labels, and masks for all batches\n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "        # Compute the average score across all tasks\n",
        "        average = torch.tensor([0.])\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:,i]==1]\n",
        "            a2 = labels_all[:, i][masks_all[:,i]==1]\n",
        "            try:\n",
        "                # Compute the metric score for the current task\n",
        "                t = metric(a2.int().cpu(), a1.cpu()).item()\n",
        "            except ValueError:\n",
        "                # If the metric cannot be computed, set the score to 0\n",
        "                t = 0\n",
        "            average += t\n",
        "    return average.item()/num_tasks"
      ],
      "metadata": {
        "id": "rx1Bd80yUTYq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    # A function to compute the loss between the model's output and the true labels\n",
        "    # mask: the binary mask indicating which samples should be included in the loss calculation\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    # Create a tensor of ones with the same shape as the output tensor (to be used as positive weights)\n",
        "    pos_weight\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
        "    # Create a loss function that applies binary cross-entropy with logits\n",
        "    # The 'none' reduction parameter is used to keep the loss for each sample separate\n",
        "    # The positive weights are used to give more weight to the positive class (to handle class imbalance)\n",
        "    loss = mask*criterion(output,label)\n",
        "    # Compute the loss for each sample, but only include the samples specified by the mask\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "vVUvNEhBUoBQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    # A function to train the model for one epoch on the given data\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() # Prepare model for training\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        # Iterate over the data loader and get the input samples\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        # Make predictions for the input samples using the model\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        # Compute the loss between the predictions and the true labels\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        # Zero the gradients of the optimizer (to prevent accumulation from previous iterations)\n",
        "        loss_train.backward()\n",
        "        # Compute the gradients of the loss with respect to the model parameters\n",
        "        optimizer.step()\n",
        "        # Update the model parameters using the computed gradients\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ],
      "metadata": {
        "id": "ZvyI7Tm_UxhE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate():\n",
        "    # A function to train and evaluate the model\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    # Create a new instance of the GNN1 model\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "    # Create an Adam optimizer to update the model parameters during training\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "    while epoch <= num_epochs:\n",
        "        # Train and evaluate the model for the specified number of epochs\n",
        "        if patience_count <= patience:\n",
        "            # Check if the patience count has been exceeded\n",
        "            model.train()\n",
        "            # Set the model to training mode\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            # Train the model on the training set for one epoch\n",
        "            model.eval()\n",
        "            # Set the model to evaluation mode\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            # Evaluate the model on the validation set and compute the validation score\n",
        "            if score_val > best_val:\n",
        "                # Check if the current validation score is better than the best validation score so far\n",
        "                best_val = score_val\n",
        "                # If so, update the best validation score and save a checkpoint of the model\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                # If not, increment the patience count\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    # Save the best model\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "    # Remove any existing best model and copy the checkpoint to the best model path\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n",
        "    # Print the final average validation score"
      ],
      "metadata": {
        "id": "NCDBMbReUyeb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluate():\n",
        "    # A function to evaluate the trained model on the test set\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    # Create a new instance of the GNN1 model\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    # Load the best model checkpoint from the best model path\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    # Load the saved model state dictionary into the final model\n",
        "    final_model.eval()\n",
        "    # Set the final model to evaluation mode\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "    # Evaluate the final model on the test set and compute the test score\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"
      ],
      "metadata": {
        "id": "jJ7gNApeU1Vw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnFZzmDMU5vz",
        "outputId": "d9daf08a-f5eb-42c5-ff9a-f5f80aad0b2c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.588 | Valid Score: 0.669\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.669 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 2/100 | Training Loss: 0.495 | Valid Score: 0.855\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.855 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 3/100 | Training Loss: 0.463 | Valid Score: 0.848\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.855 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 4/100 | Training Loss: 0.440 | Valid Score: 0.846\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.855 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 5/100 | Training Loss: 0.423 | Valid Score: 0.843\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.855 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 6/100 | Training Loss: 0.414 | Valid Score: 0.841\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.855 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 7/100 | Training Loss: 0.406 | Valid Score: 0.841\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.855 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 8/100 | Training Loss: 0.405 | Valid Score: 0.832\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.855 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 9/100 | Training Loss: 0.398 | Valid Score: 0.837\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.855 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 10/100 | Training Loss: 0.396 | Valid Score: 0.830\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.855 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 11/100 | Training Loss: 0.388 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.855 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 12/100 | Training Loss: 0.390 | Valid Score: 0.833\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 0.855 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.855 \n",
            "\n",
            "Test Score: 0.630 \n",
            "\n",
            "Execution time: 8.078 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GNN3\n"
      ],
      "metadata": {
        "id": "Dxg6IKGrXrPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = GINConv(nn.Linear(self.node_feature_size, self.hidden_size), aggregator_type='sum')\n",
        "        self.conv2 = GINConv(nn.Linear(self.hidden_size, self.num_tasks), aggregator_type='sum')\n",
        "\n",
        "    # def forward(self, g, in_feat):\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ],
      "metadata": {
        "id": "1EZQ96RCXp7_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "    # A function to compute the performance score of the model on the given data\n",
        "    # model: the trained model to be evaluated\n",
        "    # data_loader: the data loader for the validation or test set\n",
        "    model.eval()\n",
        "    metric = roc_auc_score\n",
        "    with torch.no_grad():\n",
        "        # Initialize empty tensors to store the predictions, labels, and masks\n",
        "        prediction_all= torch.empty(0)\n",
        "        labels_all= torch.empty(0)\n",
        "        masks_all= torch.empty(0)\n",
        "        # Iterate over the data loader and make predictions for each batch\n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction = torch.sigmoid(prediction)\n",
        "            # Concatenate the predictions, labels, and masks for all batches\n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "        # Compute the average score across all tasks\n",
        "        average = torch.tensor([0.])\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:,i]==1]\n",
        "            a2 = labels_all[:, i][masks_all[:,i]==1]\n",
        "            try:\n",
        "                # Compute the metric score for the current task\n",
        "                t = metric(a2.int().cpu(), a1.cpu()).item()\n",
        "            except ValueError:\n",
        "                # If the metric cannot be computed, set the score to 0\n",
        "                t = 0\n",
        "            average += t\n",
        "    return average.item()/num_tasks"
      ],
      "metadata": {
        "id": "zLHFTlNeXoE0"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "zUHAgLHFYj_E"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() # Prepare model for training\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ],
      "metadata": {
        "id": "n1mKNaxnYtBg"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    # best model save\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ],
      "metadata": {
        "id": "R4SeyqXlYv2Q"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"
      ],
      "metadata": {
        "id": "mpU4eeTYYzKa"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mRmQSbrY184",
        "outputId": "f0b8020c-b009-4cde-fb45-12c19acbddf1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.634 | Valid Score: 0.313\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.313 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 2/100 | Training Loss: 0.612 | Valid Score: 0.351\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.351 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 3/100 | Training Loss: 0.603 | Valid Score: 0.403\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.403 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 4/100 | Training Loss: 0.590 | Valid Score: 0.479\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.479 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 5/100 | Training Loss: 0.574 | Valid Score: 0.542\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.542 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 6/100 | Training Loss: 0.566 | Valid Score: 0.582\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.582 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 7/100 | Training Loss: 0.554 | Valid Score: 0.633\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.633 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 0.548 | Valid Score: 0.657\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.657 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 9/100 | Training Loss: 0.539 | Valid Score: 0.712\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.712 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 0.530 | Valid Score: 0.725\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.725 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 11/100 | Training Loss: 0.521 | Valid Score: 0.734\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.734 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 12/100 | Training Loss: 0.513 | Valid Score: 0.755\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 0.755 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 13/100 | Training Loss: 0.507 | Valid Score: 0.755\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 0.755 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 14/100 | Training Loss: 0.501 | Valid Score: 0.777\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 0.777 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 15/100 | Training Loss: 0.495 | Valid Score: 0.777\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 0.777 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 16/100 | Training Loss: 0.492 | Valid Score: 0.786\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 0.786 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 17/100 | Training Loss: 0.487 | Valid Score: 0.797\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 0.797 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 18/100 | Training Loss: 0.482 | Valid Score: 0.801\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 0.801 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 19/100 | Training Loss: 0.477 | Valid Score: 0.801\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 0.801 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 20/100 | Training Loss: 0.473 | Valid Score: 0.810\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 0.810 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 21/100 | Training Loss: 0.467 | Valid Score: 0.811\n",
            " \n",
            "Epoch: 21/100 | Best Valid Score Until Now: 0.811 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 22/100 | Training Loss: 0.467 | Valid Score: 0.814\n",
            " \n",
            "Epoch: 22/100 | Best Valid Score Until Now: 0.814 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 23/100 | Training Loss: 0.464 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 23/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 24/100 | Training Loss: 0.459 | Valid Score: 0.823\n",
            " \n",
            "Epoch: 24/100 | Best Valid Score Until Now: 0.823 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 25/100 | Training Loss: 0.456 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 25/100 | Best Valid Score Until Now: 0.823 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 26/100 | Training Loss: 0.454 | Valid Score: 0.827\n",
            " \n",
            "Epoch: 26/100 | Best Valid Score Until Now: 0.827 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 27/100 | Training Loss: 0.453 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 27/100 | Best Valid Score Until Now: 0.827 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 28/100 | Training Loss: 0.451 | Valid Score: 0.826\n",
            " \n",
            "Epoch: 28/100 | Best Valid Score Until Now: 0.827 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 29/100 | Training Loss: 0.448 | Valid Score: 0.831\n",
            " \n",
            "Epoch: 29/100 | Best Valid Score Until Now: 0.831 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 30/100 | Training Loss: 0.444 | Valid Score: 0.827\n",
            " \n",
            "Epoch: 30/100 | Best Valid Score Until Now: 0.831 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 31/100 | Training Loss: 0.446 | Valid Score: 0.829\n",
            " \n",
            "Epoch: 31/100 | Best Valid Score Until Now: 0.831 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 32/100 | Training Loss: 0.442 | Valid Score: 0.832\n",
            " \n",
            "Epoch: 32/100 | Best Valid Score Until Now: 0.832 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 33/100 | Training Loss: 0.442 | Valid Score: 0.830\n",
            " \n",
            "Epoch: 33/100 | Best Valid Score Until Now: 0.832 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 34/100 | Training Loss: 0.439 | Valid Score: 0.833\n",
            " \n",
            "Epoch: 34/100 | Best Valid Score Until Now: 0.833 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 35/100 | Training Loss: 0.438 | Valid Score: 0.829\n",
            " \n",
            "Epoch: 35/100 | Best Valid Score Until Now: 0.833 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 36/100 | Training Loss: 0.438 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 36/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 37/100 | Training Loss: 0.435 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 37/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 38/100 | Training Loss: 0.432 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 38/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 39/100 | Training Loss: 0.435 | Valid Score: 0.839\n",
            " \n",
            "Epoch: 39/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 40/100 | Training Loss: 0.431 | Valid Score: 0.836\n",
            " \n",
            "Epoch: 40/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 41/100 | Training Loss: 0.434 | Valid Score: 0.836\n",
            " \n",
            "Epoch: 41/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 42/100 | Training Loss: 0.430 | Valid Score: 0.835\n",
            " \n",
            "Epoch: 42/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 43/100 | Training Loss: 0.428 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 43/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 44/100 | Training Loss: 0.428 | Valid Score: 0.836\n",
            " \n",
            "Epoch: 44/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 45/100 | Training Loss: 0.429 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 45/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 46/100 | Training Loss: 0.427 | Valid Score: 0.832\n",
            " \n",
            "Epoch: 46/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 47/100 | Training Loss: 0.426 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 47/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 48/100 | Training Loss: 0.428 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 48/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 49/100 | Training Loss: 0.422 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 49/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.839 \n",
            "\n",
            "Test Score: 0.623 \n",
            "\n",
            "Execution time: 24.557 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GNN4"
      ],
      "metadata": {
        "id": "8guhNEzWaUjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "        self.num_heads = 4\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = GATConv(self.node_feature_size, self.hidden_size, num_heads=self.num_heads, allow_zero_in_degree=True)\n",
        "        self.conv2 = GATConv(self.hidden_size, self.num_tasks, num_heads=1, allow_zero_in_degree=True)\n",
        "    # def forward(self, g, in_feat):\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ],
      "metadata": {
        "id": "iPCF-q_-aZdk"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "def compute_score(model, data_loader, val_size, num_tasks, scaler=None):\n",
        "  model.eval()\n",
        "  loss_sum = nn.MSELoss(reduction='sum') # MSE with sum instead of mean, i.e., sum_i[(y_i)^2-(y'_i)^2]\n",
        "  final_loss = 0\n",
        "  state = torch.get_rng_state()\n",
        "  with torch.no_grad():\n",
        "            for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "                prediction = model(mol_dgl_graph, globals)\n",
        "                if scaler is not None:\n",
        "                  prediction = torch.tensor(scaler.inverse_transform(prediction.detach().cpu()))\n",
        "                  labels = torch.tensor(scaler.inverse_transform(labels.cpu()))\n",
        "                loss = loss_sum(prediction, labels)\n",
        "                final_loss += loss.item()\n",
        "            final_loss /= val_size\n",
        "            final_loss = math.sqrt(final_loss) # RMSE\n",
        "  return final_loss / num_tasks"
      ],
      "metadata": {
        "id": "l9VHN-Foatki"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "KZWh1gB4axYp"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() # Prepare model for training\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ],
      "metadata": {
        "id": "sydHMCUFaxhk"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    # best model save\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ],
      "metadata": {
        "id": "mxQy9Sl_axlE"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"
      ],
      "metadata": {
        "id": "iE7CR7obaxn4"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Odtf9Rva-Zo",
        "outputId": "b5506393-c032-4945-e0a2-f60ebd88543f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([31, 1])) that is different to the input size (torch.Size([31, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 241.741 | Valid Score: 13.877\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 13.877 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([11, 1])) that is different to the input size (torch.Size([11, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patience 1\n",
            "Epoch: 2/100 | Training Loss: 150.707 | Valid Score: 11.271\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 13.877 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 3/100 | Training Loss: 106.150 | Valid Score: 9.679\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 13.877 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 4/100 | Training Loss: 84.804 | Valid Score: 8.714\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 13.877 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 5/100 | Training Loss: 73.990 | Valid Score: 8.127\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 13.877 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 6/100 | Training Loss: 68.350 | Valid Score: 7.749\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 13.877 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 7/100 | Training Loss: 64.503 | Valid Score: 7.523\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 13.877 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 8/100 | Training Loss: 62.520 | Valid Score: 7.354\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 13.877 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 9/100 | Training Loss: 61.547 | Valid Score: 7.260\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 13.877 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 10/100 | Training Loss: 60.084 | Valid Score: 7.166\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 13.877 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 11/100 | Training Loss: 59.205 | Valid Score: 7.090\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 13.877 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 13.877 \n",
            "\n",
            "Test Score: 13.568 \n",
            "\n",
            "Execution time: 8.946 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([13, 1])) that is different to the input size (torch.Size([13, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nZ_xeRRba-c9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}