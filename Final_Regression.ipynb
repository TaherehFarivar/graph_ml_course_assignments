{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-WttHYyhXuz"
      },
      "source": [
        "## Sixth Session (Related to the Course Project)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgciSSBhXu0"
      },
      "source": [
        "---------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW5qKxrjhXu0"
      },
      "source": [
        "## Graph Regression with [Deep Graph Library (DGL)](https://docs.dgl.ai/index.html) for the graduate course \"[Graph Machine learning](https://github.com/zahta/graph_ml)\"\n",
        "\n",
        "### Dataset: PDBbind-C\n",
        "\n",
        "##### by [Zahra Taheri](https://github.com/zahta), 06 June 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77T5Lv2hhXu0"
      },
      "source": [
        "---------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoTzK-2IhXu1"
      },
      "source": [
        "### This Tutorial Is Prepared Based on the Following References\n",
        "\n",
        "- [FunQG: Molecular Representation Learning via Quotient Graphs](https://pubs.acs.org/doi/10.1021/acs.jcim.3c00445)\n",
        "- [Supporting Information of FunQG](https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c00445/suppl_file/ci3c00445_si_001.pdf)\n",
        "- [GitHub Repository of FunQG](https://github.com/hhaji/funqg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install  dgl -f https://data.dgl.ai/wheels/repo.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5IQfzqdbMY4",
        "outputId": "0e8e257d-63bb-4a57-f19b-f417f77aa799"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl\n",
            "  Downloading dgl-1.1.1-cp310-cp310-manylinux1_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install  dgl -f https://data.dgl.ai/wheels/cu113/repo.html"
      ],
      "metadata": {
        "id": "rgKaKy-sha0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html"
      ],
      "metadata": {
        "id": "03IbWGqfhlRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTEcaNXsbQFp",
        "outputId": "184de123-a87f-439c-8be0-c4b940115bab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
            "Collecting dglgo\n",
            "  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (0.7.0)\n",
            "Collecting isort>=5.10.1 (from dglgo)\n",
            "  Downloading isort-5.12.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autopep8>=1.6.0 (from dglgo)\n",
            "  Downloading autopep8-2.0.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpydoc>=1.1.0 (from dglgo)\n",
            "  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.10.9)\n",
            "Collecting ruamel.yaml>=0.17.20 (from dglgo)\n",
            "  Downloading ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from dglgo) (6.0)\n",
            "Collecting ogb>=1.3.3 (from dglgo)\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit-pypi (from dglgo)\n",
            "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.2.2)\n",
            "Collecting pycodestyle>=2.10.0 (from autopep8>=1.6.0->dglgo)\n",
            "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
            "Collecting sphinx>=4.2 (from numpydoc>=1.1.0->dglgo)\n",
            "  Downloading sphinx-7.0.1-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (4.65.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.26.16)\n",
            "Collecting outdated>=0.2.0 (from ogb>=1.3.3->dglgo)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.0->dglgo) (4.6.3)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.20->dglgo)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.4.0->dglgo) (8.1.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi->dglgo) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.3)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb>=1.3.3->dglgo)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2022.7.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.3)\n",
            "Requirement already satisfied: Pygments>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.14.0)\n",
            "Collecting docutils<0.21,>=0.18.1 (from sphinx>=4.2->numpydoc>=1.1.0->dglgo)\n",
            "  Downloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: snowballstemmer>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (0.7.13)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb>=1.3.3->dglgo) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb>=1.3.3->dglgo) (16.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb>=1.3.3->dglgo) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=a1569b2dcb78c920de790ed0fb67da86c2a66b09fb93a498c3d2cfe2f24037a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, ruamel.yaml.clib, rdkit-pypi, pycodestyle, isort, docutils, sphinx, ruamel.yaml, outdated, autopep8, numpydoc, ogb, dglgo\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.16\n",
            "    Uninstalling docutils-0.16:\n",
            "      Successfully uninstalled docutils-0.16\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 3.5.4\n",
            "    Uninstalling Sphinx-3.5.4:\n",
            "      Successfully uninstalled Sphinx-3.5.4\n",
            "Successfully installed autopep8-2.0.2 dglgo-0.0.2 docutils-0.20.1 isort-5.12.0 littleutils-0.2.2 numpydoc-1.5.0 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.10.0 rdkit-pypi-2022.9.5 ruamel.yaml-0.17.32 ruamel.yaml.clib-0.2.7 sphinx-7.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RH7Or5WjhXu1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl.function as fn\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "from torch.utils.data import DataLoader\n",
        "import cloudpickle\n",
        "from dgl.nn import GraphConv\n",
        "from dgl.nn import GINConv\n",
        "from dgl.nn import SAGEConv\n",
        "from dgl.nn import GATConv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z37D6dxGhXu1"
      },
      "source": [
        "#### Set Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g47gT5EGD_aX",
        "outputId": "1c7b263d-d82a-44a9-a6cd-5f49c5e6adeb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unzip\n",
            "  Downloading unzip-1.0.0.tar.gz (704 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unzip\n",
            "  Building wheel for unzip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unzip: filename=unzip-1.0.0-py3-none-any.whl size=1279 sha256=576716018462b8b9227b0e5acbc1d8cdf9068d10bf67f0949b3522c042490a45\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/dc/7a/f8af45bc239e7933509183f038ea8d46f3610aab82b35369f4\n",
            "Successfully built unzip\n",
            "Installing collected packages: unzip\n",
            "Successfully installed unzip-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc0AxQa33KPm",
        "outputId": "c12f7ed8-ba57-4d14-f068-393b95e4d61e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/graph_data0.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xewnPG0uELcs",
        "outputId": "27206b02-a3d0-48f7-ef65-d526d1dbfc89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/graph_data0.zip\n",
            "  inflating: scaffold_0_smiles_train.pickle  \n",
            "  inflating: scaffold_0_test.bin     \n",
            "  inflating: scaffold_0_val.bin      \n",
            "  inflating: scaffold_0_smiles_val.pickle  \n",
            "  inflating: scaffold_0_smiles_test.pickle  \n",
            "  inflating: scaffold_0_train.bin    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the current directory where the ZIP file is located.\n",
        "current_dir = \"/content/drive/MyDrive/graph_data0.zip\"\n",
        "\n",
        "# Create the path to the directory where model checkpoints will be saved.\n",
        "checkpoint_path = current_dir + \"save_models/model_checkpoints/\" + \"checkpoint\"\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the directory where the best model will be saved.\n",
        "best_model_path = current_dir + \"save_models/best_model/\"\n",
        "\n",
        "# Create a temporary folder path for data manipulation.\n",
        "folder_data_temp = current_dir + \"data_temp/\"\n",
        "\n",
        "# Remove the temporary folder if it exists, ignoring any errors if it does not.\n",
        "shutil.rmtree(folder_data_temp, ignore_errors=True)\n",
        "\n",
        "# Define the path to save the unpacked files from the ZIP archive.\n",
        "path_save = current_dir\n",
        "\n",
        "# Unpack the contents of the ZIP archive to the temporary folder.\n",
        "shutil.unpack_archive(path_save, folder_data_temp)\n"
      ],
      "metadata": {
        "id": "Oig_cfJnzMSZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4GH1uUxhXu2"
      },
      "source": [
        "#### Custom PyTorch Datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset class for regression datasets.\n",
        "\"\"\" Regression Dataset \"\"\"\n",
        "class DGLDatasetReg(torch.utils.data.Dataset):\n",
        "    def __init__(self, address, transform=None, train=True, scaler=None , scaler_regression=None):\n",
        "        # Initialize the dataset with relevant parameters.\n",
        "        self.train = train\n",
        "        self.scaler = scaler\n",
        "\n",
        "        # Load the graphs from the given binary file.\n",
        "        self.data_set, train_labels_masks_globals = dgl.load_graphs(address+\".bin\")\n",
        "\n",
        "        # Extract labels, masks, and global features for each graph.\n",
        "        num_graphs = len(self.data_set)\n",
        "        self.labels = train_labels_masks_globals[\"labels\"].view(num_graphs,-1)\n",
        "        self.masks = train_labels_masks_globals[\"masks\"].view(num_graphs,-1)\n",
        "        self.globals = train_labels_masks_globals[\"globals\"].view(num_graphs,-1)\n",
        "\n",
        "        # Store the data transformation function (if provided).\n",
        "        self.transform = transform\n",
        "        self.scaler_regression = scaler_regression\n",
        "\n",
        "    def scaler_method(self):\n",
        "        # Create and fit a standard scaler to normalize the labels during training.\n",
        "        if self.train:\n",
        "            scaler = preprocessing.StandardScaler().fit(self.labels)\n",
        "            return scaler\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of graphs in the dataset.\n",
        "        return len(self.data_set)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.scaler_regression:\n",
        "            # Apply the scaler to the labels if specified.\n",
        "            \"\"\" With Scaler \"\"\"\n",
        "            return self.data_set[idx], torch.tensor(self.scaler.transform(self.labels)[idx]).float(), self.masks[idx], self.globals[idx]\n",
        "        else:\n",
        "            # Return the data without applying the scaler.\n",
        "            \"\"\" Without Scaler \"\"\"\n",
        "            return self.data_set[idx], self.labels[idx].float(), self.masks[idx], self.globals[idx]\n"
      ],
      "metadata": {
        "id": "syHoPuso1b2e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39AeT4b4hXu2"
      },
      "source": [
        "#### Defining Train, Validation, and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the StandardScaler class for label scaling.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Define the path to the temporary data folder with a specific scaffold and index.\n",
        "path_data_temp = folder_data_temp + \"scaffold\" + \"_\" + str(0)\n",
        "\n",
        "# Create the training dataset using the DGLDatasetReg class and the specified data address.\n",
        "train_set = DGLDatasetReg(address=path_data_temp + \"_train\")\n",
        "\n",
        "# Fit the scaler using the transformed labels from the training set.\n",
        "scaler.fit(train_set.scaler_method().transform(train_set.labels))\n",
        "\n",
        "# Create the validation dataset using the DGLDatasetReg class, the specified data address, and the scaler.\n",
        "val_set = DGLDatasetReg(address=path_data_temp + \"_val\", scaler=scaler)\n",
        "\n",
        "# Create the test dataset using the DGLDatasetReg class, the specified data address, and the scaler.\n",
        "test_set = DGLDatasetReg(address=path_data_temp + \"_test\", scaler=scaler)\n",
        "\n",
        "# Print the lengths of the training, validation, and test sets.\n",
        "print(len(train_set), len(val_set), len(test_set))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aISbVq8C2bJV",
        "outputId": "7bc6e8ff-d380-445f-db58-40dd800d333f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134 16 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN3c53dRhXu3"
      },
      "source": [
        "#### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a collate function to process a batch of data samples.\n",
        "def collate(batch):\n",
        "    # Extract the graphs from the batch and create a batched graph using dgl.batch.\n",
        "    graphs = [e[0] for e in batch]\n",
        "    g = dgl.batch(graphs)\n",
        "\n",
        "    # Extract the labels from the batch and stack them into a tensor.\n",
        "    labels = [e[1] for e in batch]\n",
        "    labels = torch.stack(labels, 0)\n",
        "\n",
        "    # Extract the masks from the batch and stack them into a tensor.\n",
        "    masks = [e[2] for e in batch]\n",
        "    masks = torch.stack(masks, 0)\n",
        "\n",
        "    # Extract the global features from the batch and stack them into a tensor.\n",
        "    globals = [e[3] for e in batch]\n",
        "    globals = torch.stack(globals, 0)\n",
        "\n",
        "    # Return the batched graph, labels, masks, and globals.\n",
        "    return g, labels, masks, globals\n",
        "\n",
        "\n",
        "# Define a loader function to create data loaders for the training, validation, and test sets.\n",
        "def loader(batch_size=64):\n",
        "    # Create a data loader for the training set.\n",
        "    train_dataloader = DataLoader(train_set,\n",
        "                                  batch_size=batch_size,\n",
        "                                  collate_fn=collate,\n",
        "                                  drop_last=False,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=1)\n",
        "\n",
        "    # Create a data loader for the validation set.\n",
        "    val_dataloader = DataLoader(val_set,\n",
        "                                batch_size=batch_size,\n",
        "                                collate_fn=collate,\n",
        "                                drop_last=False,\n",
        "                                shuffle=False,\n",
        "                                num_workers=1)\n",
        "\n",
        "    # Create a data loader for the test set.\n",
        "    test_dataloader = DataLoader(test_set,\n",
        "                                 batch_size=batch_size,\n",
        "                                 collate_fn=collate,\n",
        "                                 drop_last=False,\n",
        "                                 shuffle=False,\n",
        "                                 num_workers=1)\n",
        "\n",
        "    # Return the data loaders for training, validation, and test sets.\n",
        "    return train_dataloader, val_dataloader, test_dataloader\n"
      ],
      "metadata": {
        "id": "lBsQwY2D5vqD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zHCkULH5hXu3"
      },
      "outputs": [],
      "source": [
        "# Create data loaders for the training, validation, and test sets with a batch size of 64.\n",
        "train_dataloader, val_dataloader, test_dataloader = loader(batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBRkhkI3hXu3"
      },
      "source": [
        "#### Defining A GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDru1hnyhXu3"
      },
      "source": [
        "##### Some Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CFPaFdHkhXu3"
      },
      "outputs": [],
      "source": [
        "#Bace dataset has 1 task. Some other datasets may have some more number of tasks, e.g., tox21 has 12 tasks.\n",
        "num_tasks = 1\n",
        "\n",
        "# Size of global feature of each graph\n",
        "global_size = 200\n",
        "\n",
        "# Number of epochs to train the model\n",
        "num_epochs = 100\n",
        "\n",
        "# Number of steps to wait if the model performance on the validation set does not improve\n",
        "patience = 10\n",
        "\n",
        "#Configurations to instantiate the model\n",
        "config = {\"node_feature_size\":127, \"edge_feature_size\":12, \"hidden_size\":100}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QoPRoiI5hXu3"
      },
      "outputs": [],
      "source": [
        "#Define a GNN (Graph Neural Network) class as a subclass of nn.Module\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        # Create the first GraphConv layer with input size equal to the node feature size and output size equal to the hidden size.\n",
        "        self.conv1 = GraphConv(self.node_feature_size, self.hidden_size)\n",
        "\n",
        "        # Create the second GraphConv layer with input size equal to the hidden size and output size equal to the number of tasks\n",
        "        self.conv2 = GraphConv(self.hidden_size, self.num_tasks)\n",
        "\n",
        "    # def forward(self, g, in_feat):\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzqdwrdohXu4"
      },
      "source": [
        "#### Function to Compute Score of the Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "  model.eval()\n",
        "  loss_sum = nn.MSELoss(reduction='sum') # MSE with sum instead of mean, i.e., sum_i[(y_i)^2-(y'_i)^2]\n",
        "  final_loss = 0\n",
        "  state = torch.get_rng_state()\n",
        "  with torch.no_grad():\n",
        "            for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "                prediction = model(mol_dgl_graph, globals)\n",
        "                prediction = torch.tensor(scaler.inverse_transform(prediction.detach().cpu()))\n",
        "                labels = torch.tensor(scaler.inverse_transform(labels.cpu()))\n",
        "                loss = loss_sum(prediction, labels)\n",
        "                final_loss += loss.item()\n",
        "            final_loss /= val_size\n",
        "            final_loss = math.sqrt(final_loss)\n",
        "  return final_loss / num_tasks"
      ],
      "metadata": {
        "id": "U1pu-l6hSvvl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required modules.\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define a function to compute the score using the given model, data loader, validation size, and number of tasks.\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "    # Set the model to evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    # Define the loss function as Mean Squared Error (MSE) with sum reduction.\n",
        "    loss_sum = nn.MSELoss(reduction='sum')\n",
        "\n",
        "    # Initialize the final loss variable.\n",
        "    final_loss = 0\n",
        "\n",
        "    # Get the current random number generator state.\n",
        "    state = torch.get_rng_state()\n",
        "\n",
        "    # Disable gradient calculation since we are in evaluation mode.\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the data loader.\n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            # Make predictions using the model.\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "\n",
        "            # Inverse transform the predictions using the scaler.\n",
        "            prediction = torch.tensor(scaler.inverse_transform(prediction.detach().cpu()))\n",
        "\n",
        "            # Inverse transform the labels using the scaler.\n",
        "            labels = torch.tensor(scaler.inverse_transform(labels.cpu()))\n",
        "\n",
        "            # Compute the loss between the predictions and labels.\n",
        "            loss = loss_sum(prediction, labels)\n",
        "\n",
        "            # Accumulate the loss.\n",
        "            final_loss += loss.item()\n",
        "\n",
        "        # Compute the average loss.\n",
        "        final_loss /= val_size\n",
        "\n",
        "        # Take the square root of the average loss to obtain the final score.\n",
        "        final_loss = math.sqrt(final_loss) #RMSE\n",
        "\n",
        "    # Return the final score divided by the number of tasks.\n",
        "    return final_loss / num_tasks\n"
      ],
      "metadata": {
        "id": "_7kxloWXD6CY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNOajUO7hXu4"
      },
      "source": [
        "#### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RIa0EBikhXu4"
      },
      "outputs": [],
      "source": [
        "# Define a function to compute the loss using the given output, label, mask, and number of tasks.\n",
        "def loss_func(output, label, mask, num_tasks):\n",
        "    # Create a tensor of ones with shape (1, num_tasks) as the positive weight.\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "\n",
        "    # Create a criterion using Mean Squared Error (MSE) loss with no reduction.\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "\n",
        "    # Compute the element-wise loss by multiplying the mask with the criterion output.\n",
        "    loss = mask * criterion(output, label)\n",
        "\n",
        "    # Compute the average loss by summing the masked losses and dividing by the sum of the mask.\n",
        "    loss = loss.sum() / mask.sum()\n",
        "\n",
        "    # Return the computed loss.\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8aquB5ehXu4"
      },
      "source": [
        "#### Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fgQkoc7hXu4"
      },
      "source": [
        "##### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZmR38IFxhXu4"
      },
      "outputs": [],
      "source": [
        "# Define a function to train a single epoch using the given training data loader, model, and optimizer.\n",
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    # Initialize the epoch train loss and iterations.\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "\n",
        "    # Set the model to train mode.\n",
        "    model.train()\n",
        "\n",
        "    # Iterate over the training data loader.\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        # Make predictions using the model.\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "\n",
        "        # Compute the training loss using the loss function.\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "\n",
        "        # Zero the gradients of the model parameters.\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Perform backpropagation to compute gradients.\n",
        "        loss_train.backward()\n",
        "\n",
        "        # Update the model parameters using the optimizer.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the training loss.\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "\n",
        "        # Increment the iterations count.\n",
        "        iterations += 1\n",
        "\n",
        "    # Compute the average epoch train loss.\n",
        "    epoch_train_loss /= iterations\n",
        "\n",
        "    # Return the average epoch train loss.\n",
        "    return epoch_train_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mx02YIrihXu5"
      },
      "outputs": [],
      "source": [
        "# Define a function to train and evaluate the model.\n",
        "def train_evaluate():\n",
        "    # Create a new instance of the GNN model with the given configuration.\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "\n",
        "    # Create an Adam optimizer for training the model.\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    # Initialize variables for tracking the best validation score and patience count.\n",
        "    best_val = float('inf')\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    # Continue training until reaching the maximum number of epochs.\n",
        "    while epoch <= num_epochs:\n",
        "        # Check if the patience count is within the allowed limit.\n",
        "        if patience_count <= patience:\n",
        "            # Set the model to train mode and compute the training loss for the current epoch.\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "\n",
        "            # Set the model to eval mode and compute the validation score.\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "\n",
        "            # Check if the current validation score is better than the best validation score so far.\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "\n",
        "                # Create a dictionary to store the checkpoint information.\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "\n",
        "                # Save the checkpoint to a file using cloudpickle.\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            # Print the training and validation scores for the current epoch.\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(epoch, num_epochs, loss_train, score_val))\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "    # Save the best model by copying the checkpoint directory.\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    # Print the final results.\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twE7VJvQhXu5"
      },
      "source": [
        "##### Function to compute test set score of the final saved model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OQblpT0hXu5"
      },
      "source": [
        "##### Train the model and evaluate its performance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluate():\n",
        "    # Create the final model\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "\n",
        "    # Set the path to the best model checkpoint file\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "\n",
        "    # Open the best model checkpoint file and load it\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "\n",
        "    # Load the state dictionary of the best model\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # Set the final model to evaluation mode\n",
        "    final_model.eval()\n",
        "\n",
        "    # Compute the test score\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    # Print the test score\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "\n",
        "    # Print the execution time\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"
      ],
      "metadata": {
        "id": "2Ivm_9-PIRxE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "11d-SdbahXu5",
        "outputId": "a66c3351-77b5-4005-ba0d-7d601e505fbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([6, 1])) that is different to the input size (torch.Size([6, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patience 1\n",
            "Epoch: 1/100 | Training Loss: 7231.685 | Valid Score: 58.796\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: inf \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([16, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patience 2\n",
            "Epoch: 2/100 | Training Loss: 7166.269 | Valid Score: 58.571\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 3\n",
            "Epoch: 3/100 | Training Loss: 7102.994 | Valid Score: 58.351\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 4\n",
            "Epoch: 4/100 | Training Loss: 7042.010 | Valid Score: 58.133\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 5\n",
            "Epoch: 5/100 | Training Loss: 6982.257 | Valid Score: 57.914\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 6\n",
            "Epoch: 6/100 | Training Loss: 6922.615 | Valid Score: 57.696\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 7\n",
            "Epoch: 7/100 | Training Loss: 6862.157 | Valid Score: 57.477\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 8\n",
            "Epoch: 8/100 | Training Loss: 6802.364 | Valid Score: 57.258\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 9\n",
            "Epoch: 9/100 | Training Loss: 6743.176 | Valid Score: 57.040\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 10\n",
            "Epoch: 10/100 | Training Loss: 6683.583 | Valid Score: 56.820\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: inf \n",
            "\n",
            "Test Score: 55.247 \n",
            "\n",
            "Execution time: 1.554 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([18, 1])) that is different to the input size (torch.Size([18, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ],
      "source": [
        "#This line imports the time module, which provides various time-related functions\n",
        "import time\n",
        "#This line records the current time using time.time() and assigns it to the variable start_time. It serves as the starting point for measuring the execution time\n",
        "start_time = time.time()\n",
        "#This line calls the train_evaluate() function, which is likely responsible for training and evaluating a model.\n",
        "train_evaluate()\n",
        "#This line calls the test_evaluate() function, which probably performs evaluation on a separate test dataset.\n",
        "test_evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GNN2"
      ],
      "metadata": {
        "id": "XrTzS6z3e6WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = SAGEConv(self.node_feature_size, self.hidden_size, aggregator_type='mean')\n",
        "        self.conv2 = SAGEConv(self.hidden_size, self.num_tasks, aggregator_type='mean')\n",
        "\n",
        "    # def forward(self, g, in_feat):\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ],
      "metadata": {
        "id": "aoJvb-sBe5gq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "  model.eval()\n",
        "  loss_sum = nn.MSELoss(reduction='sum') # MSE with sum instead of mean, i.e., sum_i[(y_i)^2-(y'_i)^2]\n",
        "  final_loss = 0\n",
        "  state = torch.get_rng_state()\n",
        "  with torch.no_grad():\n",
        "            for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "                prediction = model(mol_dgl_graph, globals)\n",
        "                prediction = torch.tensor(scaler.inverse_transform(prediction.detach().cpu()))\n",
        "                labels = torch.tensor(scaler.inverse_transform(labels.cpu()))\n",
        "                loss = loss_sum(prediction, labels)\n",
        "                final_loss += loss.item()\n",
        "            final_loss /= val_size\n",
        "            final_loss = math.sqrt(final_loss) # RMSE\n",
        "  return final_loss / num_tasks"
      ],
      "metadata": {
        "id": "v4wKHy2rfU7R"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "8xSvGWWufXrN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() # Prepare model for training\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ],
      "metadata": {
        "id": "R_yA88S9faVN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    # best model save\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ],
      "metadata": {
        "id": "pfR95LLIfddR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"
      ],
      "metadata": {
        "id": "tOd3VGGdfgVG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l2g4imzfnWd",
        "outputId": "c61591cc-101a-4f9c-fa80-825ca2fa2c13"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 26.925 | Valid Score: 6.431\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 6.431 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 2/100 | Training Loss: 29.046 | Valid Score: 6.353\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 6.431 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 3/100 | Training Loss: 38.460 | Valid Score: 6.276\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 6.431 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 4/100 | Training Loss: 24.076 | Valid Score: 6.201\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 6.431 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 5/100 | Training Loss: 25.473 | Valid Score: 6.127\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 6.431 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 6/100 | Training Loss: 31.716 | Valid Score: 6.054\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 6.431 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 7/100 | Training Loss: 25.288 | Valid Score: 5.980\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 6.431 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 8/100 | Training Loss: 25.859 | Valid Score: 5.908\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 6.431 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 9/100 | Training Loss: 21.750 | Valid Score: 5.837\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 6.431 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 10/100 | Training Loss: 19.770 | Valid Score: 5.768\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 6.431 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 11/100 | Training Loss: 25.627 | Valid Score: 5.699\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 6.431 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 6.431 \n",
            "\n",
            "Test Score: 5.983 \n",
            "\n",
            "Execution time: 1.374 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GNN3"
      ],
      "metadata": {
        "id": "a8Tmnwh4f66l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = SAGEConv(self.node_feature_size, self.hidden_size, aggregator_type='mean')\n",
        "        self.conv2 = SAGEConv(self.hidden_size, self.num_tasks, aggregator_type='mean')\n",
        "\n",
        "    # def forward(self, g, in_feat):\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ],
      "metadata": {
        "id": "Npal2IUWf53R"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "def compute_score(model, data_loader, val_size, num_tasks, scaler=None):\n",
        "  model.eval()\n",
        "  loss_sum = nn.MSELoss(reduction='sum') # MSE with sum instead of mean, i.e., sum_i[(y_i)^2-(y'_i)^2]\n",
        "  final_loss = 0\n",
        "  state = torch.get_rng_state()\n",
        "  with torch.no_grad():\n",
        "            for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "                prediction = model(mol_dgl_graph, globals)\n",
        "                if scaler is not None:\n",
        "                  prediction = torch.tensor(scaler.inverse_transform(prediction.detach().cpu()))\n",
        "                  labels = torch.tensor(scaler.inverse_transform(labels.cpu()))\n",
        "                loss = loss_sum(prediction, labels)\n",
        "                final_loss += loss.item()\n",
        "            final_loss /= val_size\n",
        "            final_loss = math.sqrt(final_loss) # RMSE\n",
        "  return final_loss / num_tasks"
      ],
      "metadata": {
        "id": "Hi63dXTsgQUC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "JXARjaXJgUge"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() # Prepare model for training\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ],
      "metadata": {
        "id": "SK9tkp7UgYRW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    # best model save\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ],
      "metadata": {
        "id": "C7kJ_zMAgbpf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"
      ],
      "metadata": {
        "id": "mIVuJ5hbgeNV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhrVE_m4ghI5",
        "outputId": "b589e870-8c07-462d-932a-6b10dc4ded72"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 36.900 | Valid Score: 7.426\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 7.426 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 2/100 | Training Loss: 41.333 | Valid Score: 7.346\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 7.426 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 3/100 | Training Loss: 35.836 | Valid Score: 7.267\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 7.426 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 4/100 | Training Loss: 39.173 | Valid Score: 7.188\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 7.426 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 5/100 | Training Loss: 35.245 | Valid Score: 7.109\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 7.426 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 6/100 | Training Loss: 35.816 | Valid Score: 7.030\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 7.426 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 7/100 | Training Loss: 32.535 | Valid Score: 6.952\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 7.426 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 8/100 | Training Loss: 33.585 | Valid Score: 6.875\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 7.426 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 9/100 | Training Loss: 35.483 | Valid Score: 6.798\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 7.426 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 10/100 | Training Loss: 32.252 | Valid Score: 6.721\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 7.426 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 11/100 | Training Loss: 30.421 | Valid Score: 6.645\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 7.426 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 7.426 \n",
            "\n",
            "Test Score: 6.899 \n",
            "\n",
            "Execution time: 1.458 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GNN4"
      ],
      "metadata": {
        "id": "E1y-T_yMgs7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "        self.num_heads = 4\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = GATConv(self.node_feature_size, self.hidden_size, num_heads=self.num_heads)\n",
        "        self.conv2 = GATConv(self.hidden_size, self.num_tasks, num_heads=1)\n",
        "\n",
        "    # def forward(self, g, in_feat):\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ],
      "metadata": {
        "id": "DwHeIZGUgDld"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "def compute_score(model, data_loader, val_size, num_tasks, scaler=None):\n",
        "  model.eval()\n",
        "  loss_sum = nn.MSELoss(reduction='sum') # MSE with sum instead of mean, i.e., sum_i[(y_i)^2-(y'_i)^2]\n",
        "  final_loss = 0\n",
        "  state = torch.get_rng_state()\n",
        "  with torch.no_grad():\n",
        "            for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "                prediction = model(mol_dgl_graph, globals)\n",
        "                if scaler is not None:\n",
        "                  prediction = torch.tensor(scaler.inverse_transform(prediction.detach().cpu()))\n",
        "                  labels = torch.tensor(scaler.inverse_transform(labels.cpu()))\n",
        "                loss = loss_sum(prediction, labels)\n",
        "                final_loss += loss.item()\n",
        "            final_loss /= val_size\n",
        "            final_loss = math.sqrt(final_loss) # RMSE\n",
        "  return final_loss / num_tasks"
      ],
      "metadata": {
        "id": "BncBECf5g2m1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "-dNu9mXChAX2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() # Prepare model for training\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ],
      "metadata": {
        "id": "tPwZ0EpxhD5d"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = math.inf\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    # best model save\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ],
      "metadata": {
        "id": "71PFSMmShG4h"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"
      ],
      "metadata": {
        "id": "5MtUHJhghKoN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHeIY1JvhOCN",
        "outputId": "cbb0bebe-d2eb-41fe-fc15-cb50d4faf25a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([6, 1])) that is different to the input size (torch.Size([6, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patience 1\n",
            "Epoch: 1/100 | Training Loss: 6255.016 | Valid Score: 55.058\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: inf \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([16, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patience 2\n",
            "Epoch: 2/100 | Training Loss: 6202.146 | Valid Score: 54.868\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 3\n",
            "Epoch: 3/100 | Training Loss: 6155.103 | Valid Score: 54.679\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 4\n",
            "Epoch: 4/100 | Training Loss: 6105.192 | Valid Score: 54.491\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 5\n",
            "Epoch: 5/100 | Training Loss: 6057.119 | Valid Score: 54.303\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 6\n",
            "Epoch: 6/100 | Training Loss: 6009.181 | Valid Score: 54.115\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 7\n",
            "Epoch: 7/100 | Training Loss: 5959.991 | Valid Score: 53.927\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 8\n",
            "Epoch: 8/100 | Training Loss: 5911.547 | Valid Score: 53.738\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 9\n",
            "Epoch: 9/100 | Training Loss: 5860.475 | Valid Score: 53.549\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Patience 10\n",
            "Epoch: 10/100 | Training Loss: 5815.732 | Valid Score: 53.359\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: inf \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: inf \n",
            "\n",
            "Test Score: 55.247 \n",
            "\n",
            "Execution time: 2.215 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([18, 1])) that is different to the input size (torch.Size([18, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}